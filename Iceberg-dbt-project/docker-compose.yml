services:
  minio:
    image: minio/minio:latest
    container_name: minio
    hostname: minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-your_user}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-your_password}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: curl -f http://localhost:9000/minio/health/live || exit 1
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - iceberg-net
    restart: always

  minio-init:
    image: minio/mc:latest
    container_name: minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: /bin/sh -c "mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER:-your_user} $${MINIO_ROOT_PASSWORD:-your_password} && mc mb myminio/warehouse --ignore-existing && mc mb myminio/raw-data --ignore-existing && exit 0"
    networks:
      - iceberg-net

  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    hostname: iceberg-rest
    environment:
      CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-your_user}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-your_password}
      AWS_REGION: ${AWS_REGION:-us-east-1}
    ports:
      - "8181:8181"
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - iceberg-net
    restart: always

  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    hostname: spark-master
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-your_user}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-your_password}
      AWS_REGION: ${AWS_REGION:-us-east-1}
    ports:
      - "8082:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
    networks:
      - iceberg-net
    restart: always

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    hostname: spark-worker
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-your_user}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-your_password}
      AWS_REGION: ${AWS_REGION:-us-east-1}
    ports:
      - "8081:8081"
    depends_on:
      - spark-master
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
    networks:
      - iceberg-net
    restart: always

  spark-thrift:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-thrift
    hostname: spark-thrift
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-your_user}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-your_password}
      AWS_REGION: ${AWS_REGION:-us-east-1}
    ports:
      - "10000:10000"
    depends_on:
      - spark-master
      - iceberg-rest
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
    networks:
      - iceberg-net
    restart: always
    command: >
      bash -c "
      /opt/spark/sbin/start-thriftserver.sh
      --master spark://spark-master:7077
      --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
      --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
      --conf spark.sql.catalog.iceberg.type=rest
      --conf spark.sql.catalog.iceberg.uri=http://iceberg-rest:8181
      --conf spark.sql.catalog.iceberg.io-impl=org.apache.iceberg.aws.s3.S3FileIO
      --conf spark.sql.catalog.iceberg.s3.endpoint=http://minio:9000
      --conf spark.sql.catalog.iceberg.s3.path-style-access=true
      --conf spark.sql.defaultCatalog=iceberg
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=$${AWS_ACCESS_KEY_ID:-your_user}
      --conf spark.hadoop.fs.s3a.secret.key=$${AWS_SECRET_ACCESS_KEY:-your_password}
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      --hiveconf hive.server2.thrift.port=10000
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0 &&
      tail -f /opt/spark/logs/*thrift*.out
      "

volumes:
  minio_data:

networks:
  iceberg-net:
    name: iceberg-net
    driver: bridge
